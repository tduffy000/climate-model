---
title: "Project_3_SST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ncdf4)
library(glmnet)
library(MASS)
library(biglm)
```

### Data Input

First, we must begin by loading in the data from the netCDF file format containing the measurements over each month. 
```{r init_load}
# load-in months from data folder (user-specific)
locations  <- c("../data/matchups_NPP_2018-01.nc", "../data/matchups_NPP_2018-02.nc", 
               "../data/matchups_NPP_2018-03.nc", "../data/matchups_NPP_2018-04.nc")

# pull in the variables from the files and bind together
bright_3.7 <- vector("numeric")
bright_8.6 <- vector("numeric")
bright_11  <- vector("numeric")
bright_12  <- vector("numeric")
sec_theta  <- vector("numeric")
sst_ref    <- vector("numeric")
sst_reg    <- vector("numeric")
sst_act    <- vector("numeric")
sza        <- vector("numeric")
for( loc in locations ) {
  open_file  <- nc_open(loc)
  sza        <- c(sza, ncvar_get(open_file, 'sza'))
  bright_3.7 <- c(bright_3.7, ncvar_get(open_file, 'BT_M12'))
  bright_8.6 <- c(bright_8.6, ncvar_get(open_file, 'BT_M14'))
  bright_11  <- c(bright_11, ncvar_get(open_file, 'BT_M15'))
  bright_12  <- c(bright_12, ncvar_get(open_file, 'BT_M16'))
  sec_theta  <- c(sec_theta, ncvar_get(open_file, 'vza'))
  sst_ref    <- c(sst_ref, ncvar_get(open_file, 'sst_ref'))
  sst_reg    <- c(sst_reg, ncvar_get(open_file, 'sst_reg'))
  sst_act    <- c(sst_act, ncvar_get(open_file, 'sst_insitu'))
}

# create data frame
df    <- data.frame(bright_3.7 = bright_3.7, bright_8.6 = bright_8.6, bright_11 = bright_11,
                    bright_12 = bright_12, sec_theta = sec_theta, sst_ref = sst_ref, sst_reg = sst_reg,
                    sst_act = sst_act, sza = sza)
# indicator of whether it's day time
df$is_day   <- as.integer(df$sza < 95)

# clean out memory by removing all the vectors from this step
rm(open_file, bright_3.7, bright_8.6, bright_11, bright_12, sec_theta, sst_ref, sst_reg, sst_act, sza)
gc(verbose = F)
```

```{r }
# what does df look like?
head(df)
```

### Model Introduction

Anding and Kauth (1970) found that the difference in measurement at properly selected infrared (IR) channels is proportional to the required amount of atmospheric correction.

Barton (1995) used this differential absorption between the channels which is used in all IR sea surface temperature (SST) model. In its basic form it usually represented as:

$$
T_S = aT_{\lambda_i}+\gamma(T_{\lambda_i}-T_{\lambda_j})+c
$$

where $T_s$ is estimated SST, $T_{\lambda_i}$ and $T_{\lambda_j}$ are brigthness temperature measurements in channels $\lambda_i$ and $\lambda_j$ where $i\ne j$. Therefore, the trick is to estimated which channels must be used. Both, $a$ and $c$ are constants. Finally, $\gamma$ is defined as

$$
\gamma=\frac{1-\tau_{\lambda_i}}{\tau_{\lambda_i}-\tau_{\lambda_j}}
$$

where $\tau$ is the transmittance through the atmosphere from the sea surface to the signal receiving satellite.

Though all statistical models share the above form, various modifications have been made over time to improve performance. One such model is based on the non-linear SST algorithm (NLSST) which was originally developed by Walton et al. (1998) which has the form:

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)
$$

where $T_{ref}$ is a best-first-guess SST value and $\sec\theta$ is the satellite zenith angle. 

### Part 1



### Part 2



### Part 3



### Part 4
Adding further variables and segmenting the model based on whether it's day or night using the solar zenith angel ```sza``` to threshold, we can enrich our model which then requires the estimation of 14 parameters.

In the daytime the model has the form:
$$
T_S=b_0+b_1T_{11}+b_2(T_{11}-T_{3.7}) + b_3(T_{11}-T_{8.6})+b_4(T_{11}-T_{12})\\
+(\sec\theta-1)[b_5+b_6T_{11}+b_7(T_{11}-T_{3.7})+b_8(T_{11}-T_{8.6})+b_9(T_{11}-T_{12})]\\
+T_{ref}[b_{10}T_{11}+b_{11}(T_{11}-T_{3.7})+b_{12}(T_{11}-T_{8.6})+b_{13}(T_{11}-T_{12})]
$$

And, in the evening: 
$$
T_S=b_0+b_1T_{11}+b_2(T_{11}-T_{8.6}) +b_3(T_{11}-T_{12})\\
+(\sec\theta-1)[b_4+b_5T_{11}+b_6(T_{11}-T_{8.6})+b_7(T_{11}-T_{12})]\\
+T_{ref}[b_8T_{11}+b_9(T_{11}-T_{8.6})+b_{10}(T_{11}-T_{12})]
$$

We'll need to make some modifications to our original data to include these differences.

```{r df_mod}
df$diff_11_3.7 <- df$is_day * (df$bright_11 - df$bright_3.7)  # include if daytime indicator
df$diff_11_8.6 <- df$bright_11 - df$bright_8.6
df$diff_11_12  <- df$bright_11 - df$bright_12
head(df)
```

Notice, the difference between the nighttime and daytime model is the exclusion of the $T_{11}-T_{3.7}$ channel in the evening, so using our indicator variable ```is_day``` we can simplify our implementation.

Now, we must estimate our coefficient vector $\vec{b}$ for each of the models defined above. Here we will use the shrinkage methods of LASSO and Ridge regression. 

#### Model Fitting

Generally, we know that in fitting a model with least squares we need to find the estimates $\vec{b}$ such that

$$
\vec{b}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2} 
$$
for $i$ observations and $p$ parameters. 

In penalized regression (of which both LASSO and Ridge are specific cases), we want to avoid overfitting, or adding too many $x_j$ to our model if they do not provide a significant benefit. This way we maintain a rich model while not over complicating it unless necessary. In Ridge, then we tackle the following minimization problem:
$$
\vec{b}_{Ridge}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2}+\lambda\sum_{j=1}^pb^2_j 
$$

where now we have an additional penalty term for adding a parameter to the model.

In LASSO, the minimization problem takes the form:
$$
\vec{b}_{LASSO}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2}+\lambda\sum_{j=1}^p|b_j| 
$$

Here, the goal is to predict ```sst_insitu``` with maximal accuracy as this is the actual "in the water" temperature that we are attempting to predict. We can then compare the performance of our model with the operational ```sst_reg```.

Within the ```glmnet()``` function of the ```glmnet``` library, the parameter ```alpha``` specifies the elasticnet mixing parameter. The penalty within the function is defined:
$$
\frac{(1-\alpha)}{2}\lVert\beta\rVert^2_2+\alpha\lVert\beta\rVert_1
$$

So, ```alpha=1``` simplifies to the LASSO penalty and ```alpha=0``` simplifies to the Ridge penalty. The ```glmnet()``` function requires our x variables represented as a matrix, so we'll need to perform some pre-processing to ensure it runs smoothly. We'll use different train and tests subsets of our data to get a better sense of out-of-sample performance for our models. 

```{r model_prep}
# split into train & test sets for more robust performance evaluation
set.seed(42)
TRAIN_SIZE  <- .7
train_mask  <- sample.int(n = nrow(df), size = floor(nrow(df) * TRAIN_SIZE), replace = F)
form        <- formula(sst_act ~ bright_11 + diff_11_3.7 + diff_11_8.6 + diff_11_12 + 
                     bright_11:sec_theta + diff_11_3.7:sec_theta + diff_11_8.6:sec_theta + diff_11_12:sec_theta +
                     bright_11:sst_ref + diff_11_3.7:sst_ref + diff_11_8.6:sst_ref + diff_11_12:sst_ref)
X_train     <- model.matrix(form, df[train_mask,])
y_train     <- as.matrix(df$sst_act[train_mask], ncol = 1)
```

Now, we can fit both our LASSO and Ridge models on our training set and then make predictions on our excluded test set.

```{r fit}
fit_lasso <- glmnet::glmnet(X_train, y_train, alpha = 1)
fit_ridge <- glmnet::glmnet(X_train, y_train, alpha = 0)
rm(X_train, y_train) # memory management
```

```{r pred}
rm(X_train, y_train) # memory management
X_test         <- model.matrix(form, df[-train_mask,])
y_test         <- as.matrix(df$sst_act[-train_mask], ncol = 1)

# predict 
predict_df         <- data.frame(y_actual = y_test)
predict_df$y_lasso <- predict(fit_lasso, X_test, s = min(fit_lasso$lambda), type = "response")
predict_df$y_ridge <- predict(fit_ridge, X_test, s = min(fit_ridge$lambda), type = "response")
rm(X_test, y_test); gc(verbose=F) # memory management
head(predict_df)
```

What about their prediction accuracy on the test set? How is it compared to the error from the operational model? Let's use the root-mean squared error over our test set as the metric, which has the form:

$$
RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2}
$$

where $y$ is the actual value and $\hat{y}$ is the corresponding model predicted value.

```{r error}
rmse <- function(y_act, y_pred) {
  return( sqrt( length(y_pred)^-1 * sum((y_pred - y_act)^2) ))
}

# add normalized function
nrmse <- function(y_act, y_pred) {
  return( rmse(y_act, y_pred) / mean(y_act) )
}

error_list <- list(
  base_rmse = rmse(df$sst_act[-train_mask], df$sst_reg[-train_mask]),
  lasso_rmse = rmse(df$sst_act[-train_mask], predict_df$y_lasso),
  ridge_rmse = rmse(df$sst_act[-train_mask], predict_df$y_ridge),
  base_nrmse = nrmse(df$sst_act[-train_mask], df$sst_reg[-train_mask]), 
  lasso_nrmse = nrmse(df$sst_act[-train_mask], predict_df$y_lasso),
  ridge_rmse = nrmse(df$sst_act[-train_mask], predict_df$y_ridge)
)
error_list
```

What do the solution pathways for both versions look like?

```{r model_vis}
plot(fit_lasso)
plot(fit_ridge)
```

What about across the lambda parameters?

```{r lambda}
plot(fit_lasso, xvar = "lambda")
plot(fit_ridge, xvar = "lambda")
```

In order to ensure there is not some interaction present that might threaten the robustness of our model, we can investigate the distribution of our residuals, $\hat{y_i}-y_i$ for both the models. 

```{r hist}
hist(predict_df$y_ridge - predict_df$y_actual, main = "Ridge Regression Residuals", xlab = "Distance from Actual",
     col = 3)
hist(predict_df$y_lasso - predict_df$y_actual, main = "LASSO Regression Residuals", xlab = "Distance from Actual",
     col = 5)
```

In both cases, they appear to be relatively normal, which should provide us with further confidence that our model is justifiable. 

### Part 5
Returning to our model that segments depending on time of day from Part 4, perhaps the usage of forward or backward selection would perform just as well, or better. The forward selection process begins with a null and then iteratively adds a variable that maximizes the criteria in a step-wise fashion (the backward selection beginning with the full model and going in reverse). The two criterion used here will be the Akaike Information Criterion (AIC) and the Bayesian Information Criteria (BIC).

In the case of AIC we are fitting to minimize the following:

$$
AIC = 2p-2\ln(\hat{L})
$$

where $p$ is the number of parameters within our model and $\hat{L}$ is the maximum of the likelihood for the candidate model. The BIC has a similar form, but different penalty. It is defined:

$$
BIC = p\ln(n)-2\ln(\hat{L})
$$

where $n$ is the number of observations.

In order to perform the following procedures in an environment with limited storage resources, we'll have to subset the original data significantly given that ```R``` tends to store all of its objects in memory. Even only using 0.05% of our data will still leave us with 181,171 observations, which should be more than enough for our purposes.

Let's fit our models, by first fitting the full model (which is required by ```step()```:

```{r full_fit}
SUBSET_SIZE <- .005
sub_mask    <- sample.int(n = nrow(df), size = floor(nrow(df) * SUBSET_SIZE), replace = F)
sub_df      <- df[sub_mask,]
test_mask   <- sample.int(n = nrow(df[-sub_mask,]), size = nrow(df[-sub_mask,]) * SUBSET_SIZE, replace = F)
test_df     <- df[setdiff(sub_mask, test_mask),]

full_lm     <- lm(form, data = sub_df)
summary(full_lm)
```

```{r stepwise}
fit_forward  <- MASS::stepAIC(full_lm, direction = "forward")
fit_backward <- MASS::stepAIC(full_lm, direction = "backward")
```

```{r anova}
fit_forward$anova; fit_backward$anova
```

And we can now compare them to our performance using the LASSO and Ridge results from Part 4.

```{r stepwise_performance}
# predict step-wise on test set
y_forward  <- predict(fit_forward, test_df, type = "response")
y_backward <- predict(fit_backward, test_df, type = "response")

list(
  forward_rmse = rmse(test_df$sst_act, y_forward),
  backward_rmse = rmse(test_df$sst_act, y_backward),
  forward_nrmse = nrmse(test_df$sst_act, y_forward),
  backward_rmse = nrmse(test_df$sst_act, y_backward)
)
```

So, this procedure generates a model that is more accurate than those determined by the LASSO & Ridge procedures (though again fit on a much smaller subset of data). We did not test the error on the entire dataset, which may explain the robust performance. That said, it appears to approximate the error of the current model.