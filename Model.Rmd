---
title: "Project_3_SST"
output: pdf_document
author:
- Tony Hoang
- Carlos Martinez
- Md Ayub Ali Sarker
- Thomas Duffy
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ncdf4)
library(glmnet)
library(MASS)
library(biglm)
library(gtools)
library(leaps)
library(caret)
library(dplyr)
library(ggplot2)
```

### Data Input

First, we must begin by loading in the data from the netCDF file format containing the measurements over each month. 
```{r init_load}
# load-in months from data folder (user-specific)
locations  <- c("../data/matchups_NPP_2018-01.nc", "../data/matchups_NPP_2018-02.nc",
                "../data/matchups_NPP_2018-03.nc","../data/matchups_NPP_2018-04.nc")

# pull in the variables from the files and bind together
bright_3.7 <- vector("numeric")
bright_4   <- vector("numeric")
bright_8.6 <- vector("numeric")
bright_11  <- vector("numeric")
bright_12  <- vector("numeric")
sec_theta  <- vector("numeric")
sst_ref    <- vector("numeric")
sst_reg    <- vector("numeric")
sst_act    <- vector("numeric")
sza        <- vector("numeric")
for( loc in locations ) {
  open_file  <- nc_open(loc)
  sza        <- c(sza, ncvar_get(open_file, 'sza'))
  bright_3.7 <- c(bright_3.7, ncvar_get(open_file, 'BT_M12'))
  bright_4   <- c(bright_4, ncvar_get(open_file, 'BT_M13'))
  bright_8.6 <- c(bright_8.6, ncvar_get(open_file, 'BT_M14'))
  bright_11  <- c(bright_11, ncvar_get(open_file, 'BT_M15'))
  bright_12  <- c(bright_12, ncvar_get(open_file, 'BT_M16'))
  sec_theta  <- c(sec_theta, ncvar_get(open_file, 'vza'))
  sst_ref    <- c(sst_ref, ncvar_get(open_file, 'sst_ref'))
  sst_reg    <- c(sst_reg, ncvar_get(open_file, 'sst_reg'))
  sst_act    <- c(sst_act, ncvar_get(open_file, 'sst_insitu'))
}

# create data frame
df        <- data.frame(bright_3.7 = bright_3.7, bright_4 = bright_4, bright_8.6 = bright_8.6, bright_11 = bright_11,
                    bright_12 = bright_12, sec_theta = sec_theta, sst_ref = sst_ref, sst_reg = sst_reg,
                    sst_act = sst_act, sza = sza)
# indicator of whether it's day time
df$is_day <- as.integer(df$sza < 95)

# clean out memory by removing all the vectors from this step
rm(open_file, bright_3.7, bright_4, bright_8.6, bright_11, bright_12, sec_theta, sst_ref, sst_reg, sst_act, sza)
```

```{r }
# what does df look like?
head(df)
```

### Model Introduction

Anding and Kauth (1970) found that the difference in measurement at properly selected infrared (IR) channels is proportional to the required amount of atmospheric correction.

Barton (1995) used this differential absorption between the channels which is used in all IR sea surface temperature (SST) model. In its basic form it usually represented as:

$$
T_S = aT_{\lambda_i}+\gamma(T_{\lambda_i}-T_{\lambda_j})+c
$$

where $T_s$ is estimated SST, $T_{\lambda_i}$ and $T_{\lambda_j}$ are brigthness temperature measurements in channels $\lambda_i$ and $\lambda_j$ where $i\ne j$. Therefore, the trick is to estimated which channels must be used. Both, $a$ and $c$ are constants. Finally, $\gamma$ is defined as

$$
\gamma=\frac{1-\tau_{\lambda_i}}{\tau_{\lambda_i}-\tau_{\lambda_j}}
$$

where $\tau$ is the transmittance through the atmosphere from the sea surface to the signal receiving satellite.

Though all statistical models share the above form, various modifications have been made over time to improve performance. One such model is based on the non-linear SST algorithm (NLSST) which was originally developed by Walton et al. (1998) which has the form:

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)
$$

where $T_{ref}$ is a best-first-guess SST value and $\sec\theta$ is the satellite zenith angle. 

For the below analysis, given a limited amount of resources available, we must pull out random subsets from our data in order to fit the models. Below, we'll construct a mask of training & testing data (they have no intersection) of approximately 10% of the original data. Given that ```R``` stores its objects in RAM, it's not possible to do fittings much larger.

```{r}
set.seed(42)
TRAIN_SIZE <- .1
train_mask <- sample.int(n = nrow(df), size = floor(nrow(df) * TRAIN_SIZE), replace = F)
TEST_SIZE  <- .1
test_mask  <- setdiff(sample.int(n = nrow(df), size = floor(nrow(df) * TEST_SIZE), replace = F), train_mask)

# how large is our test set?
length(test_mask)/nrow(df)
```

### Part 1

<< DISCUSSION >>

```{r}
x    <- c('bright_3.7','bright_4','bright_8.6','bright_11','bright_12')
perm <- gtools::permutations(n = 5, r = 2, v = x)

# Generating all possible linear formula with pair of I and J (20 formula total)
formula <- list(length = nrow(perm))
for (index in 1:nrow(perm)) {
  formula[[index]] <- as.formula(paste("sst_act~", toString(perm[index,1]),
                     "+I(", toString(perm[index,1]),"-", toString(perm[index,2]), ")", sep = ""))
}
head(formula)
```

```{r}
nfolds      <- 5
fold.labels <- sample(rep(1:nfolds, length = floor(nrow(df)) * TEST_SIZE), replace = F)

# Setting up MSE matrix with formula as column names and n-fold as row label
rmse.models           <- matrix(NA, nrow = nfolds, ncol = nrow(perm))
colnames(rmse.models) <- as.character(formula)

# Running CV:
for (fold in 1:nfolds) {
  test.rows       <- setdiff(which(fold.labels == fold), train_mask)
  for (form in formula) {
    current.model <- lm(formula = form, data = df[test.rows,])
    predictions   <- predict(current.model, newdata = df[test_mask,])
    rmse.models[fold, as.character(c(form))] <- sqrt(mean((df[test_mask,]$sst_act - predictions)^2))
  }
}
rm(current.model, predictions)

# CV results:
cv.results <- as.matrix(colMeans(rmse.models))
cv.results
```

```{r}
plot(1:20, cv.results, xlab = "models", ylab = "RMSE", main = "RMSE results for CV with 5 fold", type = "b")
```

### Part 2

Walton et al. (1988) proposed the following model to estimate sst:

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)
$$

Here, we must find the best channels $T_{\lambda_i}$ and $T_{\lambda_j}$ which is possible to do by exhaustive search just the small search space. Then, we can pick the best model by investigating the root mean squared error and performance on the outstanding test set.

Setting up all possible pairs of I and J for the model. There are 20 possible combinations
```{r}
x    <- c('bright_3.7','bright_4','bright_8.6','bright_11','bright_12')
perm <- gtools::permutations( n = 5, r = 2, v = x)

#Generating all possible linear formula with pair of I and J (20 formula total)
formula <- list(length = nrow(perm))
for (index in 1:nrow(perm)) {
  formula[[index]] <- as.formula(c(paste("sst_act~", toString(perm[index,1]),
                     "+I(", toString(perm[index,1]),"-", toString(perm[index,2]),"):sec_theta", sep = "")))
}
head(formula)
```

Setting up n-fold Cross-Validation:
```{r}
nfolds <- 5
fold.labels <- sample(rep(1:nfolds, length = floor(nrow(df)) * TEST_SIZE), replace = F)

#Setting up RMSE matrix with formula as column names and n-fold as row label
rmse.models <- matrix(NA, nrow = nfolds, ncol = nrow(perm))
colnames(rmse.models) <- as.character(formula)

#Running CV:
for (fold in 1:nfolds) {
  test.rows <- setdiff(which(fold.labels==fold), train_mask)
  for (form in formula) {
    current.model <- lm(formula = form, data = df[test.rows,])
    predictions <- predict(current.model, newdata = df[test_mask,])
    rmse.models[fold, as.character(c(form))] <- sqrt(mean((df[test_mask,]$sst_act - predictions)^2))
  }
}
rm(current.model, predictions)

#CV results:
cv.results <- as.matrix(colMeans(rmse.models))
cv.results
```

Plotting the result for model comparison:
```{r}
plot(1:20, cv.results, xlab = "models", ylab = "RMSE", main = "RMSE results for CV with 5 fold", type = "b")
```

The model that has the lowest RMSE is with I = 11 and J = 8.6
```{r}
names(cv.results[which.min(cv.results),])
rmse.model <- cv.results[which.min(cv.results)]
rmse.model
```

Checking the performance of the operational model, measured by RMSE:
```{r}
rmse.Top <- sqrt(mean((df$sst_reg - df$sst_act)^2))
rmse.Top 
```

The RMSE of the operational model is much lower than the selected model. Therefore, the selected model is not performing as well.

### Part 3

To perform a subset selection of the predicitors for the model below, 

$$
T_S=b_0+b_1T_{\lambda_i}+b_2(T_{\lambda_i}-T_{\lambda_j})T_{ref}+b_3(T_{\lambda_i}-T_{\lambda_j})(\sec\theta-1)+b_4T_{ref}
$$

we will use the best subset algorithm, which gives the total of possible models that can be created from $p$ variables selecting $k$ number of predictors.

$$
{p \choose k}=\frac{p!}{k!(p-k)!}
$$

#### Data Preparation

Due to our model consist of three predictors and two of the predictors is the difference between two variables. To calculate the total number of variables in our data, it is necessary to violate the Best subset selection algorithm. This algorithm is limited to analyze sets of pairs. It does not hold for pair repetition. In our model each variable is used twiced if and only if both the variables have different wavelengths. In other words, the model establishes a different order each time a variable is used. To assert the total of possible models, mathematically this can be expressed as:

$$
k!{p \choose k} = \frac{p!}{(p-k)!}
$$

To account for the order of each variable used, it is convenient to estimate the amount of possible by permutation instead of combinations. In total we will have 20 possible models, from which the model with the highest adjusted $R^2_{adj}$ will be selected. 

```{r Data Preparation}
# permutation of all possible variables combination for the model
x     <- c("bright_3.7", "bright_4", "bright_8.6", "bright_11", "bright_12")
p     <- 5
k     <- 2
permu <- data.frame(gtools::permutations(n = p, r = k, v = x))
```

```{r}
# Dimensions of the sample
print(dim(df[train_mask,]))
# Dimensions of the permutation
print(dim(permu))
# Permutation data frame
print(permu)
```

#### Model Selection
To select the best model, the goal is to identify the model with maximum adjusted $R^{2}$. We developed an algorithm to extract $R^{2}$ from each possible model.

```{r Model Selection}
# Storage lists set up
adj_rs <- vector("numeric")
os_rmse <- vector("numeric")

# loop through permutations of model
for (index in 1:nrow(permu)) {
  formula <- as.formula(paste("sst_act ~", toString(permu[index, 1]), "+I(",toString(permu[index,1]), "-",                                         toString(permu[index, 2]), ")+ I(", toString(permu[index,1]), "-",
                              toString(permu[index, 2]), "):sec_theta", "+ sst_ref", sep=""))
  model               <- lm(formula = formula, data = df[train_mask,])
  prediction          <- predict(model, df[test_mask,], type = "response")
  adj_rs[[index]]     <- summary(model)$adj.r.squared
  os_rmse[[index]]    <- sqrt(mean( (df[test_mask,]$sst_act - prediction)^2 ))
}
rm(model, prediction) # memory management
  
dff <- data.frame(permutations = permu, adjusted_R2 = adj_rs, OS_rmse = os_rmse)
dff
```

```{r plot_1}
plot(dff$adjusted_R2, type = "p", ylab = "Adj. R2", main = "Variables Selection", col = "blue")
par(new = TRUE)
plot(dff$OS_rmse, type = "p", xaxt = "n", yaxt = "n", ylab = "", xlab = "", col = "red", lty = 2)
axis(side = 4)
mtext("RMSE", side = 4)
legend("left", c("Adj R2", "RMSE"), col = c("blue", "red"), lty = c(1,2))
axis(side = 1, at = seq(1,20))
grid(21)
```

```{r plot_2}
ddff <- dff %>% filter(dff$adjusted_R2 > 0.999, dff$OS_rmse < 0.056)
plot(dff$adjusted_R2, type = "p", ylab = "Adj. R2", main = "Variables Selection", col = "blue")
par(new = TRUE)
plot(dff$OS_rmse, type = "p", xaxt = "n", yaxt = "n", ylab = "", xlab = "", col = "red", lty = 2)
axis(side = 4)
mtext("RMSE", side = 4)
legend("right", c("Adj R2", "RMSE"), col = c("blue", "red"), lty = c(1,2))
axis(side = 1, at = seq(1,8))
grid(20)
```

Based on the results, it can be concluded all models provide the same accuracy of prediction. However, after careful analysis, denoting the highest adjusted $R^2$ and lowest $RMSE$ the best variable pairs correspond to model number 11, which is $i=T_{3.7}$ and $j=T_{r}$.

### Part 4
Adding further variables and segmenting the model based on whether it's day or night using the solar zenith angel ```sza``` to threshold, we can enrich our model which then requires the estimation of 14 parameters.

In the daytime the model has the form:
$$
T_S=b_0+b_1T_{11}+b_2(T_{11}-T_{3.7}) + b_3(T_{11}-T_{8.6})+b_4(T_{11}-T_{12})\\
+(\sec\theta-1)[b_5+b_6T_{11}+b_7(T_{11}-T_{3.7})+b_8(T_{11}-T_{8.6})+b_9(T_{11}-T_{12})]\\
+T_{ref}[b_{10}T_{11}+b_{11}(T_{11}-T_{3.7})+b_{12}(T_{11}-T_{8.6})+b_{13}(T_{11}-T_{12})]
$$

And, in the evening: 
$$
T_S=b_0+b_1T_{11}+b_2(T_{11}-T_{8.6}) +b_3(T_{11}-T_{12})\\
+(\sec\theta-1)[b_4+b_5T_{11}+b_6(T_{11}-T_{8.6})+b_7(T_{11}-T_{12})]\\
+T_{ref}[b_8T_{11}+b_9(T_{11}-T_{8.6})+b_{10}(T_{11}-T_{12})]
$$

We'll need to make some modifications to our original data to include these differences.

```{r df_mod}
df$diff_11_3.7 <- df$is_day * (df$bright_11 - df$bright_3.7)  # include if daytime indicator
df$diff_11_8.6 <- df$bright_11 - df$bright_8.6
df$diff_11_12  <- df$bright_11 - df$bright_12
head(df)
```

Notice, the difference between the nighttime and daytime model is the exclusion of the $T_{11}-T_{3.7}$ channel in the evening, so using our indicator variable ```is_day``` we can simplify our implementation.

Now, we must estimate our coefficient vector $\vec{b}$ for each of the models defined above. Here we will use the shrinkage methods of LASSO and Ridge regression. 

#### Model Fitting

Generally, we know that in fitting a model with least squares we need to find the estimates $\vec{b}$ such that

$$
\vec{b}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2} 
$$
for $i$ observations and $p$ parameters. 

In penalized regression (of which both LASSO and Ridge are specific cases), we want to avoid overfitting, or adding too many $x_j$ to our model if they do not provide a significant benefit. This way we maintain a rich model while not over complicating it unless necessary. In Ridge, then we tackle the following minimization problem:
$$
\vec{b}_{Ridge}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2}+\lambda\sum_{j=1}^pb^2_j 
$$

where now we have an additional penalty term for adding a parameter to the model.

In LASSO, the minimization problem takes the form:
$$
\vec{b}_{LASSO}=\arg \min_b\sum_{i=1}^n{(y_i-b_0-\sum_{j=1}^pb_jx_{ij})^2}+\lambda\sum_{j=1}^p|b_j| 
$$

Here, the goal is to predict ```sst_insitu``` with maximal accuracy as this is the actual "in the water" temperature that we are attempting to predict. We can then compare the performance of our model with the operational ```sst_reg```.

Within the ```glmnet()``` function of the ```glmnet``` library, the parameter ```alpha``` specifies the elasticnet mixing parameter. The penalty within the function is defined:
$$
\frac{(1-\alpha)}{2}\lVert\beta\rVert^2_2+\alpha\lVert\beta\rVert_1
$$

So, ```alpha=1``` simplifies to the LASSO penalty and ```alpha=0``` simplifies to the Ridge penalty. The ```glmnet()``` function requires our x variables represented as a matrix, so we'll need to perform some pre-processing to ensure it runs smoothly. We'll use different train and tests subsets of our data to get a better sense of out-of-sample performance for our models. 

```{r model_prep}
# split into train & test sets for more robust performance evaluation
form        <- formula(sst_act ~ bright_11 + diff_11_3.7 + diff_11_8.6 + diff_11_12 + 
                     bright_11:sec_theta + diff_11_3.7:sec_theta + diff_11_8.6:sec_theta + diff_11_12:sec_theta +
                     bright_11:sst_ref + diff_11_3.7:sst_ref + diff_11_8.6:sst_ref + diff_11_12:sst_ref)
X_train     <- model.matrix(form, df[train_mask,])
y_train     <- as.matrix(df$sst_act[train_mask], ncol = 1)
```

Now, we can fit both our LASSO and Ridge models on our training set and then make predictions on our excluded test set.

```{r fit}
fit_lasso <- glmnet::glmnet(X_train, y_train, alpha = 1)
fit_ridge <- glmnet::glmnet(X_train, y_train, alpha = 0)
rm(X_train, y_train) # memory management

list( 
  min_lambda_lasso_model = coef(fit_lasso, s = min(s = min(fit_lasso$lambda))),
  min_lambda_ridge_model = coef(fit_ridge, s = min(s = min(fit_ridge$lambda)))
)
```

```{r pred}
X_test         <- model.matrix(form, df[test_mask,])
y_test         <- as.matrix(df$sst_act[test_mask], ncol = 1)

# predict 
predict_df         <- data.frame(y_actual = y_test)
predict_df$y_lasso <- predict(fit_lasso, X_test, s = min(fit_lasso$lambda), type = "response")
predict_df$y_ridge <- predict(fit_ridge, X_test, s = min(fit_ridge$lambda), type = "response")
rm(X_test, y_test) # memory management
```

What about their prediction accuracy on the test set? How is it compared to the error from the operational model? Let's use the root-mean squared error over our test set as the metric, which has the form:

$$
RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y_i})^2}
$$

where $y$ is the actual value and $\hat{y}$ is the corresponding model predicted value.

```{r error_fun}
rmse <- function(y_act, y_pred) {
  return( sqrt( length(y_pred)^-1 * sum((y_pred - y_act)^2) ))
}

nrmse <- function(y_act, y_pred) {
  return( rmse(y_act, y_pred) / mean(y_act) )
}
```

```{r lasso_ridge_error}
error_list <- list(
  base_rmse = rmse(df$sst_act[test_mask], df$sst_reg[test_mask]),
  lasso_rmse = rmse(df$sst_act[test_mask], predict_df$y_lasso),
  ridge_rmse = rmse(df$sst_act[test_mask], predict_df$y_ridge),
  base_nrmse = nrmse(df$sst_act[test_mask], df$sst_reg[test_mask]), 
  lasso_nrmse = nrmse(df$sst_act[test_mask], predict_df$y_lasso),
  ridge_rmse = nrmse(df$sst_act[test_mask], predict_df$y_ridge)
)
error_list
```

What coefficients are pronounced as we tune our lambda hyperparameter?

```{r lambda}
plot(fit_lasso, xvar = "lambda")
plot(fit_ridge, xvar = "lambda")
```

In order to ensure there is not some interaction present that might threaten the robustness of our model, we can investigate the distribution of our residuals, $\hat{y_i}-y_i$ for both the models. 

```{r hist}
hist(predict_df$y_ridge - predict_df$y_actual, main = "Ridge Regression Residuals", xlab = "Distance from Actual",
     col = 3)
hist(predict_df$y_lasso - predict_df$y_actual, main = "LASSO Regression Residuals", xlab = "Distance from Actual",
     col = 5)
```

In both cases, they appear to be relatively normal, which should provide us with further confidence that our model is justifiable. 

### Part 5 
Returning to our model that segments depending on time of day from Part 4, perhaps the usage of forward or backward selection would perform just as well, or better. The forward selection process begins with a null model and then iteratively adds variables which maximizes the criteria in a step-wise fashion (the backward selection beginning with the full model and going in reverse). The criterion used here will be the Akaike Information Criterion (AIC), the Bayesian Information Criteria (BIC), and Mallow's $C_p$.

In the case of AIC we are fitting to minimize the following:

$$
AIC = 2p-2\ln(\hat{L})
$$

where $p$ is the number of parameters within our model and $\hat{L}$ is the maximum of the likelihood for the candidate model. The BIC has a similar form, but different penalty. It is defined:

$$
BIC = p\ln(n)-2\ln(\hat{L})
$$

where $n$ is the number of observations.

Finally, Mallow's $C_p$ is defined as

$$
C_p = \frac{SSE_p}{MSE} - n + 2p
$$

where $SSE_p$ is the error sum of squares of the sub-model with $p$ predictors, $n$ is the sample size, and $S^2$ is the residual mean square $MSE$ on the complete set of $k$ predictors. 

Now, we can fit our model once again using the ```leaps``` library. The summary provided by this function will show both BIC and Mallow's $C_p$ statistic for each iteration in the model. We can also calculate the in-sample RMSE from the metrics as RSS is provided.

```{r backward_selection}
step_lm <- leaps::regsubsets(form, data = df[train_mask,], method = "backward", nvmax = 12)
plot(step_lm)
```

```{r backward_int}
step_adj_r2 <- summary(step_lm)$adjr2
step_bic    <- summary(step_lm)$bic
step_cp     <- summary(step_lm)$cp

step_results <- data.frame(model_size = 1:12, bic = step_bic, mallow_cp = step_cp, adj_r2 = step_adj_r2, 
                           rmse = sqrt((nrow(df[train_mask,]))^-1 * summary(step_lm)$rss))
step_results
```

```{r}
ggplot2::ggplot(step_results, aes(x = model_size)) + 
  geom_line(aes(y = bic), col = 4) +
  labs(title = "BIC over Increasing Model Size in Backward Selection", 
       x = "Model Size (excluding Intercept)", y = "BIC")
```

Finally, we can investigate the best model, but it should be noted that here each step is statistically significant enough that if we were using automated model selection based on any of the above criteria we would likely end up with the full model containing all 12 variables. Here, both the 11 and 12 variable models achieve the same BIC score. 

```{r}
# according to BIC metric what's our best model?
best_bic <- which.min(summary(step_lm)$bic)

# what are the coefficients of this model (#11)?
coef(step_lm, best_bic)
```

### Part 6

Using the object we formed in Part 5, we can investigate how the model at each step would perform on our outstanding test set that constitutes just over 9% of the overall dataset.

```{r stepwise}
step_os_rmse <- vector("numeric", length = 12)
X_test       <- model.matrix(form, data = df[test_mask,])

for(n_var in 1:12) {
  coef_n              <- coef(step_lm, n_var)
  pred_n              <- as.vector(X_test[,names(coef_n)] %*% coef_n)
  step_os_rmse[n_var] <- rmse(df$sst_act[test_mask], pred_n) 
}
rm(X_test, step_lm)

data.frame(model_size = 1:12, os_rmse = step_os_rmse)
```

Our results in Part 5 indicated that either 11 or 12 variables would be the best performing models. These results seem to confirm that, our error slightly decreases as we add more terms throughout our stepwise procedure. Seemingly, we are not paying a penalty for additional model complexity. Of course, 12 variables is still a rather small number in comparison to the enormous amount of observations available to us.

### Part 7

Given that we do not have a sample size problem, which is a situation we'd prefer to use bootstrapping, we can consider the methodology of k-fold cross-validation again. We have shown its robustness in both Part 2 and 3 before. The idea of cross-validation is that of training on one partition of our data and validating the model on a separate partition. The $k$ in $k$-fold specifies the number of partitions that our data is split up into for these purposes.

```{r cv_stepwise}
# we're including cross-validation so can concatenate train & test masks
train_control <- caret::trainControl(method = "cv", number = 5)
step_model    <- caret::train(form, data = df[c(train_mask, test_mask),], method = "leapBackward", 
                              tuneGrid = data.frame(nvmax = 1:12), trControl = train_control)
step_model$results
```

```{r cv_stepwise_plot}
# train set error
plot(step_model)
```

```{r}
print(step_model$bestTune)
```

```{r cv_step_predict}
coef(step_model$finalModel, 12)
```

So, this procedure generates a model that is more accurate than those determined by the LASSO & Ridge procedures (though again fit on a much smaller subset of data). We did not test the error on the entire dataset, which may explain the robust performance. That said, it appears to approximate the error of the current model, perhaps performing slightly better. It should be noted using either forward or backward selection does not remove any of the variables from the fully enriched model form we began with. This result is inline with what we found in Part 6. 